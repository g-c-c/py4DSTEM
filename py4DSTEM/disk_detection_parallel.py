from time import time, sleep
import ipyparallel as ipp
import dask
import distributed
import os
import pickle
import numpy as np
import h5py

import py4DSTEM
from py4DSTEM.process.braggdiskdetection import find_Bragg_disks_single_DP_FK
from py4DSTEM.process.braggdiskdetection import PointListArray
from py4DSTEM.process.braggdiskdetection import print_progress_bar

def _find_Bragg_disks_single_DP_FK(DP, probe_kernel_FT,
                                  corrPower = 1,
                                  sigma = 2,
                                  edgeBoundary = 20,
                                  minRelativeIntensity = 0.005,
                                  minPeakSpacing = 60,
                                  maxNumPeaks = 70,
                                  subpixel = 'poly',
                                  upsample_factor = 4,
                                  return_cc = False,
                                  peaks = None):
    """
    Finds the Bragg disks in DP by cross, hybrid, or phase correlation with probe_kernel_FT.

    After taking the cross/hybrid/phase correlation, a gaussian smoothing is applied
    with standard deviation sigma, and all local maxima are found. Detected peaks within
    edgeBoundary pixels of the diffraction plane edges are then discarded. Next, peaks with
    intensities less than minRelativeIntensity of the brightest peak in the correaltion are
    discarded. Then peaks which are within a distance of minPeakSpacing of their nearest neighbor
    peak are found, and in each such pair the peak with the lesser correlation intensities is
    removed. Finally, if the number of peaks remaining exceeds maxNumPeaks, only the maxNumPeaks
    peaks with the highest correlation intensity are retained.

    IMPORTANT NOTE: the argument probe_kernel_FT is related to the probe kernels generated by
    functions like get_probe_kernel() by:

            probe_kernel_FT = np.conj(np.fft.fft2(probe_kernel))

    if this function is simply passed a probe kernel, the results will not be meaningful! To run
    on a single DP while passing the real space probe kernel as an argument, use
    find_Bragg_disks_single_DP().

    Accepts:
        DP                   (ndarray) a diffraction pattern
        probe_kernel_FT      (ndarray) the vacuum probe template, in Fourier space. Related to the
                             real space probe kernel by probe_kernel_FT = F(probe_kernel)*, where F
                             indicates a Fourier Transform and * indicates complex conjugation.
        corrPower            (float between 0 and 1, inclusive) the cross correlation power. A
                             value of 1 corresponds to a cross correaltion, and 0 corresponds to a
                             phase correlation, with intermediate values giving various hybrids.
        sigma                (float) the standard deviation for the gaussian smoothing applied to
                             the cross correlation
        edgeBoundary         (int) minimum acceptable distance from the DP edge, in pixels
        minRelativeIntensity (float) the minimum acceptable correlation peak intensity, relative to
                             the intensity of the brightest peak
        minPeakSpacing       (float) the minimum acceptable spacing between detected peaks
        maxNumPeaks          (int) the maximum number of peaks to return
        subpixel             (str)          'none': no subpixel fitting
                                    default 'poly': polynomial interpolation of correlogram peaks
                                                    (fairly fast but not very accurate)
                                            'multicorr': uses the multicorr algorithm with 
                                                        DFT upsampling
        upsample_factor      (int) upsampling factor for subpixel fitting (only used when subpixel='multicorr')
        return_cc            (bool) if True, return the cross correlation
        peaks                (PointList) For internal use.
                             If peaks is None, the PointList of peak positions is created here.
                             If peaks is not None, it is the PointList that detected peaks are added
                             to, and must have the appropriate coords ('qx','qy','intensity').

    Returns:
        peaks                (PointList) the Bragg peak positions and correlation intensities
    """
    assert subpixel in [ 'none', 'poly', 'multicorr' ], "Unrecognized subpixel option {}, subpixel must be 'none', 'poly', or 'multicorr'".format(subpixel)

    import numpy
    import scipy.ndimage.filters
    import py4DSTEM.file.datastructure
    import py4DSTEM.process.utils
    import py4DSTEM.process.utils.multicorr

    if subpixel == 'none':
        cc = py4DSTEM.process.utils.get_cross_correlation_fk(DP, probe_kernel_FT, corrPower)
        cc = numpy.maximum(cc,0)
        maxima_x,maxima_y,maxima_int = py4DSTEM.process.utils.get_maxima_2D(cc, sigma=sigma,
                                                     edgeBoundary=edgeBoundary,
                                                     minRelativeIntensity=minRelativeIntensity,
                                                     minSpacing=minPeakSpacing,
                                                     maxNumPeaks=maxNumPeaks,
                                                     subpixel=False)
    elif subpixel == 'poly':
        cc = py4DSTEM.process.utils.get_cross_correlation_fk(DP, probe_kernel_FT, corrPower)
        cc = numpy.maximum(cc,0)
        maxima_x,maxima_y,maxima_int = py4DSTEM.process.utils.get_maxima_2D(cc, sigma=sigma,
                                                     edgeBoundary=edgeBoundary,
                                                     minRelativeIntensity=minRelativeIntensity,
                                                     minSpacing=minPeakSpacing,
                                                     maxNumPeaks=maxNumPeaks,
                                                     subpixel=True)
    else:
        # Multicorr subpixel:
        m = numpy.fft.fft2(DP) * probe_kernel_FT
        ccc = numpy.abs(m)**(corrPower) * numpy.exp(1j*numpy.angle(m))

        cc = numpy.maximum(numpy.real(numpy.fft.ifft2(ccc)),0)

        maxima_x,maxima_y,maxima_int = py4DSTEM.process.utils.get_maxima_2D(cc, sigma=sigma,
                                                     edgeBoundary=edgeBoundary,
                                                     minRelativeIntensity=minRelativeIntensity,
                                                     minSpacing=minPeakSpacing,
                                                     maxNumPeaks=maxNumPeaks,
                                                     subpixel=True)

        # Use the DFT upsample to refine the detected peaks (but not the intensity)
        for ipeak in range(len(maxima_x)):
            xyShift = numpy.array((maxima_x[ipeak],maxima_y[ipeak]))
            # we actually have to lose some precision and go down to half-pixel
            # accuracy. this could also be done by a single upsampling at factor 2
            # instead of get_maxima_2D.
            xyShift[0] = numpy.round(xyShift[0] * 2) / 2
            xyShift[1] = numpy.round(xyShift[1] * 2) / 2

            subShift = py4DSTEM.process.utils.multicorr.upsampled_correlation(ccc,upsample_factor,xyShift)
            maxima_x[ipeak]=subShift[0]
            maxima_y[ipeak]=subShift[1]

    # Make peaks PointList
    if peaks is None:
        coords = [('qx',float),('qy',float),('intensity',float)]
        peaks = py4DSTEM.file.datastructure.PointList(coordinates=coords)
    else:
        assert(isinstance(peaks,py4DSTEM.file.datastructure.PointList))
    peaks.add_tuple_of_nparrays((maxima_x,maxima_y,maxima_int))

    if return_cc:
        return peaks, gaussian_filter(cc,sigma)
    else:
        return peaks


def _process_chunk_ipp(_f, start, end, path_to_static, coords, path_to_data):
    import pickle
    import datetime
    import numpy
    #import h5py
    import sys
    
    t0 = datetime.datetime.utcnow()
    
    with open(path_to_static, 'rb') as infile:
        inputs = pickle.load(infile)
    
    t1 = datetime.datetime.utcnow()
    
    data4d = numpy.load(path_to_data, mmap_mode='r')
    #h5f = h5py.File(path_to_data, 'r')
    #data4d = h5f['data4d'][:]
    #h5f.close()
    
    slices = [data4d[x[0],x[1],:,:] for x in coords]
    
    t2 = datetime.datetime.utcnow()
        
    results = [(coords[n][0], coords[n][1], _f(slices[n], *inputs).data) for n in range(end - start)]
    
    t3 = datetime.datetime.utcnow()
    
    #with open('/global/homes/m/mhenders/ncem/process_chunk_{}_{}.txt'.format(start, end), 'w') as chunkTime:
    #    chunkTime.write("number of slices - {}\nbytes - {}\nstart - {}\ntime to unpickle inputs - {}\ntime to unpickle data - {}\ntime to process data - {}\n".format(
    #        len(slices), sum([x.nbytes for x in slices]), t0, t1 - t0, t2 - t1, t3 - t2))
    
    path_to_output = os.path.join(os.path.expandvars("$SCRATCH"), "{}_{}.data".format(start, end))
    with open(path_to_output, 'wb') as data_file:
        pickle.dump(results, data_file)
    
    return path_to_output


def find_Bragg_disks_ipp(datacube, probe,
                      corrPower = 1,
                      sigma = 2,
                      edgeBoundary = 20,
                      minRelativeIntensity = 0.005,
                      minPeakSpacing = 60,
                      maxNumPeaks = 70,
                      subpixel = 'poly',
                      upsample_factor = 4,
                      verbose = False,
                      cluster_id = None):
    """
    Finds the Bragg disks in all diffraction patterns of datacube by cross, hybrid, or phase
    correlation with probe.

    Accepts:
        DP                   (ndarray) a diffraction pattern
        probe                (ndarray) the vacuum probe template, in real space.
        corrPower            (float between 0 and 1, inclusive) the cross correlation power. A
                             value of 1 corresponds to a cross correaltion, and 0 corresponds to a
                             phase correlation, with intermediate values giving various hybrids.
        sigma                (float) the standard deviation for the gaussian smoothing applied to
                             the cross correlation
        edgeBoundary         (int) minimum acceptable distance from the DP edge, in pixels
        minRelativeIntensity (float) the minimum acceptable correlation peak intensity, relative to
                             the intensity of the brightest peak
        minPeakSpacing       (float) the minimum acceptable spacing between detected peaks
        maxNumPeaks          (int) the maximum number of peaks to return
        subpixel             (str)          'none': no subpixel fitting
                                    default 'poly': polynomial interpolation of correlogram peaks
                                                    (fairly fast but not very accurate)
                                            'multicorr': uses the multicorr algorithm with 
                                                        DFT upsampling
        upsample_factor      (int) upsampling factor for subpixel fitting (only used when subpixel='multicorr')
        verbose              (bool) if True, prints completion updates

    Returns:
        peaks                (PointListArray) the Bragg peak positions and correlation intensities
    """    
    # Make the peaks PointListArray
    coords = [('qx',float),('qy',float),('intensity',float)]
    peaks = PointListArray(coordinates=coords, shape=(datacube.R_Nx, datacube.R_Ny))

    # Get the probe kernel FT
    probe_kernel_FT = np.conj(np.fft.fft2(probe))

    # Make the peaks PointListArray
    coords = [('qx',float),('qy',float),('intensity',float)]
    peaks = PointListArray(coordinates=coords, shape=(datacube.R_Nx, datacube.R_Ny))

    # Get the probe kernel FT
    probe_kernel_FT = np.conj(np.fft.fft2(probe))

    if cluster_id is None:
        # Loop over all diffraction patterns
        t0 = time()
        for Rx in range(datacube.R_Nx):
            for Ry in range(datacube.R_Ny):
                if verbose:
                    print_progress_bar(Rx*datacube.R_Ny+Ry+1, datacube.R_Nx*datacube.R_Ny,
                                       prefix='Analyzing:', suffix='Complete', length=50)
                DP = datacube.data4D[Rx,Ry,:,:]
                find_Bragg_disks_single_DP_FK(DP, probe_kernel_FT,
                                              corrPower = corrPower,
                                              sigma = sigma,
                                              edgeBoundary = edgeBoundary,
                                              minRelativeIntensity = minRelativeIntensity,
                                              minPeakSpacing = minPeakSpacing,
                                              maxNumPeaks = maxNumPeaks,
                                              subpixel = subpixel,
                                              upsample_factor = upsample_factor,
                                              peaks = peaks.get_pointlist(Rx,Ry))
        t = time()-t0
        print("Analyzed {} diffraction patterns in {}h {}m {}s".format(datacube.R_N, int(t/3600),
                                                                       int(t/60), int(t%60)))
    else:
        t0 = time()
        c = ipp.Client(cluster_id=cluster_id, timeout=30)
        lastengine = len(c.ids) - 1

        inputs_list = [
            probe_kernel_FT,
            corrPower,
            sigma,
            edgeBoundary,
            minRelativeIntensity,
            minPeakSpacing,
            maxNumPeaks,
            subpixel,
            upsample_factor
        ]
        
        t_00 = time()
        # write out static inputs
        path_to_inputs = os.path.join(os.path.expandvars("$SCRATCH"), "{}.inputs".format(cluster_id))
        with open(path_to_inputs, 'wb') as inputs_file:
            pickle.dump(inputs_list, inputs_file)
        t_inputs_save = time() - t_00
        print("Serialize input values : {}".format(t_inputs_save))
        
        t_01 = time()
        path_to_data = os.path.join(os.path.expandvars("$SCRATCH"), "{}.datacube.npy".format(cluster_id))
        #if not os.path.exists(path_to_data):
        np.save(path_to_data, datacube.data4D)
        #h5f = h5py.File(path_to_data, 'w')
        #h5f.create_dataset('data4d', data=datacube.data4D)
        #h5f.close()
        t_datacube_save = time() - t_01
        print("Serialize datacube : {}".format(t_datacube_save))
        
        t_write_inputs = time() - t0        
        print("Serialize inputs phase : {}".format(t_write_inputs))
        
        results = []        
        t1 = time()
        total = int(datacube.R_Nx * datacube.R_Ny)
        chunkSize = int(total / len(c.ids))
        
        while chunkSize * len(c.ids) < total:
            chunkSize += 1

        indices = []
        for Rx in range(datacube.R_Nx):
            for Ry in range(datacube.R_Ny):
                indices.append((Rx, Ry))
        
        start = 0
        for engine in range(len(c.ids)):
            if start + chunkSize < total - 1:
                end = start + chunkSize
            else:
                end = total
            
            #path_to_data = os.path.join(os.path.expandvars("$SCRATCH"), "{}.datacube.{}".format(cluster_id, start))
            #with open(path_to_data, 'wb') as outfile:
            #    pickle.dump([datacube.data4D[x[0],x[1],:,:] for x in indices[start:end]], outfile)
            
            results.append(
                c[engine].apply(
                    _process_chunk_ipp,
                    _find_Bragg_disks_single_DP_FK,
                    start,
                    end,
                    path_to_inputs,
                    indices[start:end],
                    path_to_data
                )
            )
            
            if end == total:
                break
            else:
                start = end
        t_submit = time() - t1
        print("Submit phase : {}".format(t_submit))
        
        t2 = time()
        c.wait(jobs=results)
        
        for i in range(len(results)):
            with open(results[i].get(), 'rb') as f:
                data_chunk = pickle.load(f)
            
            for Rx, Ry, data in data_chunk:
                peaks.get_pointlist(Rx, Ry).add_dataarray(data)
        
        t_copy = time() - t2
        print("Gather phase : {}".format(t_copy))
        
        t = time()-t0
        print("Analyzed {} diffraction patterns in {}h {}m {}s".format(datacube.R_N, int(t/3600),
                                                                       int(t/60), int(t%60)))

    return peaks


def _process_chunk_dask(_f, start, end, path_to_static, coords, path_to_inputs):
    
    import pickle
    #import numpy
    
    with open(path_to_inputs, 'rb') as infile:
        slices = pickle.load(infile)
    
    with open(path_to_static, 'rb') as infile:
        inputs = pickle.load(infile)
    
    #dc = numpy.load(path_to_data, mmap_mode='r')

    results = [(coords[n][0], coords[n][1], _f(slices[n], *inputs).data) for n in range(end - start)]
    return results


def _find_Bragg_disks_dask(datacube, probe,
                      corrPower = 1,
                      sigma = 2,
                      edgeBoundary = 20,
                      minRelativeIntensity = 0.005,
                      minPeakSpacing = 60,
                      maxNumPeaks = 70,
                      subpixel = 'poly',
                      upsample_factor = 4,
                      verbose = False,
                      dask_client = None):
    """
    Finds the Bragg disks in all diffraction patterns of datacube by cross, hybrid, or phase
    correlation with probe.

    Accepts:
        DP                   (ndarray) a diffraction pattern
        probe                (ndarray) the vacuum probe template, in real space.
        corrPower            (float between 0 and 1, inclusive) the cross correlation power. A
                             value of 1 corresponds to a cross correaltion, and 0 corresponds to a
                             phase correlation, with intermediate values giving various hybrids.
        sigma                (float) the standard deviation for the gaussian smoothing applied to
                             the cross correlation
        edgeBoundary         (int) minimum acceptable distance from the DP edge, in pixels
        minRelativeIntensity (float) the minimum acceptable correlation peak intensity, relative to
                             the intensity of the brightest peak
        minPeakSpacing       (float) the minimum acceptable spacing between detected peaks
        maxNumPeaks          (int) the maximum number of peaks to return
        subpixel             (str)          'none': no subpixel fitting
                                    default 'poly': polynomial interpolation of correlogram peaks
                                                    (fairly fast but not very accurate)
                                            'multicorr': uses the multicorr algorithm with 
                                                        DFT upsampling
        upsample_factor      (int) upsampling factor for subpixel fitting (only used when subpixel='multicorr')
        verbose              (bool) if True, prints completion updates

    Returns:
        peaks                (PointListArray) the Bragg peak positions and correlation intensities
    """    # Make the peaks PointListArray
    coords = [('qx',float),('qy',float),('intensity',float)]
    peaks = PointListArray(coordinates=coords, shape=(datacube.R_Nx, datacube.R_Ny))

    # Get the probe kernel FT
    probe_kernel_FT = np.conj(np.fft.fft2(probe))

    # Make the peaks PointListArray
    coords = [('qx',float),('qy',float),('intensity',float)]
    peaks = PointListArray(coordinates=coords, shape=(datacube.R_Nx, datacube.R_Ny))

    # Get the probe kernel FT
    probe_kernel_FT = np.conj(np.fft.fft2(probe))

    if dask_client is None:
        # Loop over all diffraction patterns
        for Rx in range(datacube.R_Nx):
            for Ry in range(datacube.R_Ny):
                if verbose:
                    print_progress_bar(Rx*datacube.R_Ny+Ry+1, datacube.R_Nx*datacube.R_Ny,
                                       prefix='Analyzing:', suffix='Complete', length=50)
                DP = datacube.data4D[Rx,Ry,:,:]
                find_Bragg_disks_single_DP_FK(DP, probe_kernel_FT,
                                              corrPower = corrPower,
                                              sigma = sigma,
                                              edgeBoundary = edgeBoundary,
                                              minRelativeIntensity = minRelativeIntensity,
                                              minPeakSpacing = minPeakSpacing,
                                              maxNumPeaks = maxNumPeaks,
                                              subpixel = subpixel,
                                              upsample_factor = upsample_factor,
                                              peaks = peaks.get_pointlist(Rx,Ry))
    else:
        t0 = time()

        inputs_list = [
            probe_kernel_FT,
            corrPower,
            sigma,
            edgeBoundary,
            minRelativeIntensity,
            minPeakSpacing,
            maxNumPeaks,
            subpixel,
            upsample_factor
        ]
        
        # write out static inputs
        path_to_inputs = os.path.join(os.path.expandvars("$SCRATCH"), "{}.inputs".format(dask_client.id))
        with open(path_to_inputs, 'wb') as inputs_file:
            pickle.dump(inputs_list, inputs_file)
        t_inputs_save = time() - t0
        print("Serialize input values : {}".format(t_inputs_save))
        
        submits = []
        results = []        
        t1 = time()
        total = int(datacube.R_Nx * datacube.R_Ny)
        chunkSize = int(total / len(client.ncores()))
        
        while chunkSize * len(client.ncores()) < total:
            chunkSize += 1

        indices = []
        for Rx in range(datacube.R_Nx):
            for Ry in range(datacube.R_Ny):
                indices.append((Rx, Ry))
        
        start = 0
        for engine in range(len(client.ncores())):
            if start + chunkSize < total - 1:
                end = start + chunkSize
            else:
                end = total
            
            dyn_inputs_list = [datacube.data4D[x[0],x[1],:,:] for x in indices[start:end]]
            
            path_to_dynamic_inputs = os.path.join(os.path.expandvars("$SCRATCH"), "{}.inputs_{}".format(dask_client.id, engine))
            with open(path_to_dynamic_inputs, 'wb') as inputs_file:
                pickle.dump(dyn_inputs_list, inputs_file)
            
            submits.append(
                dask_client.submit(
                    _process_chunk,
                    _find_Bragg_disks_single_DP_FK,
                    start,
                    end,
                    path_to_inputs,
                    indices[start:end],
                    path_to_dynamic_inputs
                )
            )
            
            if end == total:
                break
            else:
                start = end
        t_submit = time() - t1
        print("Submit phase : {}".format(t_submit))
        
        t2 = time()
        # let all futures complete
        #distributed.wait(submits)
        # collect results
        results = dask_client.gather(submits, direct=True)
        t2_a_end = time()
        print("Time to gather results : {}".format(t2_a_end - t2))
        
        t2_b_start = time()
        # copy into peaks
        for i in range(len(results)):
            for Rx, Ry, data in results[i]:
                peaks.get_pointlist(Rx, Ry).add_dataarray(data)        
        t2_b_end = time()
        print("Time to copy into peaks : {}".format(t2_b_end - t2_b_start))
        
        t_copy = time() - t2
        print("Gather phase : {}".format(t_copy))
        
        t = time()-t0
        print("Analyzed {} diffraction patterns in {}h {}m {}s".format(datacube.R_N, int(t/3600),
                                                                       int(t/60), int(t%60)))
        
        #values = []

        # submit all computations
        #t0 = time()        
        #for Rx in range(datacube.R_Nx):
        #    for Ry in range(datacube.R_Ny):
        #        DP = datacube.data4D[Rx,Ry,:,:]
        #        values.append(
        #            dask_client.submit(_find_Bragg_disks_single_DP_FK, 
        #                DP, probe_kernel_FT, corrPower, sigma, edgeBoundary, 
        #                minRelativeIntensity, minPeakSpacing, maxNumPeaks, subpixel
        #            )
        #        )

        # collect results and copy into numpy arrays
        #i = 0
        #for Rx in range(datacube.R_Nx):
        #    for Ry in range(datacube.R_Ny):
        #        if verbose:
        #            print_progress_bar(Rx*datacube.R_Ny+Ry+1, datacube.R_Nx*datacube.R_Ny,
        #                               prefix='Analyzing:', suffix='Complete', length=50)
        #
        #        peaks.get_pointlist(Rx, Ry).data = dask_client.gather(values[i]).data
        #        i += 1

    return peaks
