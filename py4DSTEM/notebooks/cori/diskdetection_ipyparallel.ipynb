{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module: braggdiskdetection\n",
    "\n",
    "This module contains functions finding the positions of the Bragg disks in a 4DSTEM scan.  Generally this will involve two steps: getting a vacuum probe, then finding the Bragg disks using the vacuum probe as a template. \n",
    "\n",
    "## Submodule: diskdetection\n",
    "\n",
    "The notebook demos functions related to finding the Bragg disks.  Using a vacuum probe as a template - i.e. a convolution kernel - a cross correlation (or phase or hybrid correlation) is taken between each DP and the template, and the positions and intensities of all local corraltion maxima are used to identify the Bragg disks.  Erroneous peaks are filtered out with several types of threshold.  Detected Bragg disks are generally stored in PointLists (when run on only selected DPs) or PointListArrays (when run on a full DataCube).\n",
    "\n",
    "This notebook demos:\n",
    "* Disk detection on single or selected diffraction patterns\n",
    "* Disk detection on all diffraction patterns\n",
    "* Additional filtering of detected Bragg disks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages, load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py4DSTEM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from py4DSTEM.process.braggdiskdetection import find_Bragg_disks_single_DP\n",
    "from py4DSTEM.process.braggdiskdetection import find_Bragg_disks_selected\n",
    "from py4DSTEM.process.braggdiskdetection import find_Bragg_disks\n",
    "from py4DSTEM.process.braggdiskdetection import threshold_Braggpeaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, sleep\n",
    "import ipyparallel as ipp\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from py4DSTEM.process.braggdiskdetection import find_Bragg_disks_single_DP_FK\n",
    "from py4DSTEM.process.braggdiskdetection import PointListArray\n",
    "from py4DSTEM.process.braggdiskdetection import print_progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "#fp = \"/home/ben/Data/20180905_FePO4_unlithiated/raw/Stack1_57x47+30nmss_spot 8_0p05s_CL=600_alpha=0p48_300kV_bin4.dm4\"\n",
    "#fp = \"/Users/Ben/Work/NCEM/Projects/py4DSTEM/sample_data/20180905_FePO4_unlithiated/Stack2_60x60+30nmss_spot 8_0p05s_CL=600_alpha=0p48_300kV_bin4.dm3\"\n",
    "\n",
    "fp = \"/global/u2/m/mhenders/ncem/Stack2_60x60+30nmss_spot 8_0p05s_CL=600_alpha=0p48_300kV_bin4.h5\"\n",
    "dc = py4DSTEM.file.readwrite.read(fp)\n",
    "dc.set_scan_shape(47,57)\n",
    "dc.data4D = np.roll(dc.data4D,-2,1) # Correct for acquisition wrap-around error\n",
    "\n",
    "# Load the template\n",
    "#fp_probetemplate = \"/home/ben/Data/20180905_FePO4_unlithiated/processing/vacuum_probe_kernel.h5\"\n",
    "#fp = \"/Users/Ben/Work/NCEM/Projects/py4DSTEM/sample_data/20180905_FePO4_unlithiated/processing/vacuum_probe_kernel.h5\"\n",
    "fp_probetemplate = \"/global/u2/m/mhenders/ncem/vacuum_probe_kernel.h5\"\n",
    "browser = py4DSTEM.file.readwrite.FileBrowser(fp_probetemplate, rawdatacube=dc)\n",
    "browser.show_dataobjects()\n",
    "probe_kernel = browser.get_dataobject(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a DP\n",
    "\n",
    "Rx=20\n",
    "Ry=25\n",
    "power=0.3\n",
    "\n",
    "DP = dc.data4D[Rx,Ry,:,:]\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,12))\n",
    "ax1.matshow(np.average(dc.data4D,axis=(2,3)))\n",
    "ax1.scatter(Ry,Rx,color='r')\n",
    "ax2.matshow(DP**power)\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get peaks\n",
    "\n",
    "corrPower = 0.9\n",
    "sigma = 2\n",
    "edgeBoundary = 20\n",
    "maxNumPeaks = 70\n",
    "minPeakSpacing = 30\n",
    "minRelativeIntensity = 0.005\n",
    "\n",
    "peaks = find_Bragg_disks_single_DP(DP, probe_kernel.data2D,\n",
    "                                   corrPower=corrPower,\n",
    "                                   sigma=sigma,\n",
    "                                   edgeBoundary=edgeBoundary,\n",
    "                                   minRelativeIntensity=minRelativeIntensity,\n",
    "                                   minPeakSpacing=minPeakSpacing,\n",
    "                                   maxNumPeaks=maxNumPeaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "\n",
    "power=0.3\n",
    "size_scale_factor = 500       # Set to zero to make all points the same size\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,12))\n",
    "ax1.matshow(np.average(dc.data4D,axis=(2,3)))\n",
    "ax1.scatter(Ry,Rx,color='r')\n",
    "ax2.matshow(DP**power)\n",
    "ax2.scatter(peaks.data['qy'],peaks.data['qx'],color='r',s=size_scale_factor*peaks.data['intensity']/np.max(peaks.data['intensity']))\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Several DPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few DPs\n",
    "\n",
    "Rxs=(20,31,18)\n",
    "Rys=(25,31,10)\n",
    "power=0.3\n",
    "\n",
    "fig,((ax11,ax12),(ax21,ax22))=plt.subplots(2,2,figsize=(12,12))\n",
    "ax11.matshow(np.average(dc.data4D,axis=(2,3)))\n",
    "ax11.scatter(Rys,Rxs,color=('r','yellow','deepskyblue'))\n",
    "ax12.matshow(dc.data4D[Rxs[0],Rys[0],:,:]**power)\n",
    "ax21.matshow(dc.data4D[Rxs[1],Rys[1],:,:]**power)\n",
    "ax22.matshow(dc.data4D[Rxs[2],Rys[2],:,:]**power)\n",
    "\n",
    "ax11.axis('off')\n",
    "ax12.axis('off')\n",
    "ax21.axis('off')\n",
    "ax22.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get peaks\n",
    "\n",
    "corrPower = 0.8\n",
    "sigma = 2\n",
    "edgeBoundary = 20\n",
    "maxNumPeaks = 70\n",
    "minPeakSpacing = 50\n",
    "minRelativeIntensity = 0.001\n",
    "\n",
    "peaks = find_Bragg_disks_selected(dc, probe_kernel.data2D, Rxs, Rys,\n",
    "                                  corrPower=corrPower,\n",
    "                                  sigma=sigma,\n",
    "                                  edgeBoundary=edgeBoundary,\n",
    "                                  minRelativeIntensity=minRelativeIntensity,\n",
    "                                  minPeakSpacing=minPeakSpacing,\n",
    "                                  maxNumPeaks=maxNumPeaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "\n",
    "power=0.3\n",
    "size_scale_factor = 500       # Set to zero to make all points the same size\n",
    "\n",
    "fig,((ax11,ax12),(ax21,ax22))=plt.subplots(2,2,figsize=(12,12))\n",
    "ax11.matshow(np.average(dc.data4D,axis=(2,3)))\n",
    "ax11.scatter(Rys,Rxs,color=('r','g','b'))\n",
    "ax12.matshow(dc.data4D[Rxs[0],Rys[0],:,:]**power)\n",
    "ax21.matshow(dc.data4D[Rxs[1],Rys[1],:,:]**power)\n",
    "ax22.matshow(dc.data4D[Rxs[2],Rys[2],:,:]**power)\n",
    "\n",
    "if size_scale_factor == 0:\n",
    "    ax12.scatter(peaks[0].data['qy'],peaks[0].data['qx'],color='r')\n",
    "    ax21.scatter(peaks[1].data['qy'],peaks[1].data['qx'],color='g')\n",
    "    ax22.scatter(peaks[2].data['qy'],peaks[2].data['qx'],color='b')\n",
    "else:\n",
    "    ax12.scatter(peaks[0].data['qy'],peaks[0].data['qx'],color='r',s=size_scale_factor*peaks[0].data['intensity']/np.max(peaks[0].data['intensity']))\n",
    "    ax21.scatter(peaks[1].data['qy'],peaks[1].data['qx'],color='g',s=size_scale_factor*peaks[1].data['intensity']/np.max(peaks[1].data['intensity']))\n",
    "    ax22.scatter(peaks[2].data['qy'],peaks[2].data['qx'],color='b',s=size_scale_factor*peaks[2].data['intensity']/np.max(peaks[2].data['intensity']))\n",
    "\n",
    "\n",
    "ax11.axis('off')\n",
    "ax12.axis('off')\n",
    "ax21.axis('off')\n",
    "ax22.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_Bragg_disks_single_DP_FK(DP, probe_kernel_FT,\n",
    "                                   corrPower = 1,\n",
    "                                   sigma = 2,\n",
    "                                   edgeBoundary = 20,\n",
    "                                   minRelativeIntensity = 0.005,\n",
    "                                   minPeakSpacing = 60,\n",
    "                                   maxNumPeaks = 70,\n",
    "                                   subpixel = True,\n",
    "                                   return_cc = False,\n",
    "                                   peaks = None):\n",
    "    \"\"\"\n",
    "    Finds the Bragg disks in DP by cross, hybrid, or phase correlation with probe_kernel_FT.\n",
    "\n",
    "    After taking the cross/hybrid/phase correlation, a gaussian smoothing is applied\n",
    "    with standard deviation sigma, and all local maxima are found. Detected peaks within\n",
    "    edgeBoundary pixels of the diffraction plane edges are then discarded. Next, peaks with\n",
    "    intensities less than minRelativeIntensity of the brightest peak in the correaltion are\n",
    "    discarded. Then peaks which are within a distance of minPeakSpacing of their nearest neighbor\n",
    "    peak are found, and in each such pair the peak with the lesser correlation intensities is\n",
    "    removed. Finally, if the number of peaks remaining exceeds maxNumPeaks, only the maxNumPeaks\n",
    "    peaks with the highest correlation intensity are retained.\n",
    "\n",
    "    IMPORTANT NOTE: the argument probe_kernel_FT is related to the probe kernels generated by\n",
    "    functions like get_probe_kernel() by:\n",
    "\n",
    "            probe_kernel_FT = np.conj(np.fft.fft2(probe_kernel))\n",
    "\n",
    "    if this function is simply passed a probe kernel, the results will not be meaningful! To run\n",
    "    on a single DP while passing the real space probe kernel as an argument, use\n",
    "    find_Bragg_disks_single_DP().\n",
    "\n",
    "    Accepts:\n",
    "        DP                   (ndarray) a diffraction pattern\n",
    "        probe_kernel_FT      (ndarray) the vacuum probe template, in Fourier space. Related to the\n",
    "                             real space probe kernel by probe_kernel_FT = F(probe_kernel)*, where F\n",
    "                             indicates a Fourier Transform and * indicates complex conjugation.\n",
    "        corrPower            (float between 0 and 1, inclusive) the cross correlation power. A\n",
    "                             value of 1 corresponds to a cross correaltion, and 0 corresponds to a\n",
    "                             phase correlation, with intermediate values giving various hybrids.\n",
    "        sigma                (float) the standard deviation for the gaussian smoothing applied to\n",
    "                             the cross correlation\n",
    "        edgeBoundary         (int) minimum acceptable distance from the DP edge, in pixels\n",
    "        minRelativeIntensity (float) the minimum acceptable correlation peak intensity, relative to\n",
    "                             the intensity of the brightest peak\n",
    "        minPeakSpacing       (float) the minimum acceptable spacing between detected peaks\n",
    "        maxNumPeaks          (int) the maximum number of peaks to return\n",
    "        subpixel             (bool) if True, perform subpixel fitting to detected maxima\n",
    "        return_cc            (bool) if True, return the cross correlation\n",
    "        peaks                (PointList) For internal use.\n",
    "                             If peaks is None, the PointList of peak positions is created here.\n",
    "                             If peaks is not None, it is the PointList that detected peaks are added\n",
    "                             to, and must have the appropriate coords ('qx','qy','intensity').\n",
    "\n",
    "    Returns:\n",
    "        peaks                (PointList) the Bragg peak positions and correlation intensities\n",
    "    \"\"\"\n",
    "    import numpy\n",
    "    import scipy.ndimage.filters\n",
    "    import py4DSTEM.file.datastructure\n",
    "    import py4DSTEM.process.utils\n",
    "    \n",
    "    # Get cross correlation\n",
    "    cc = py4DSTEM.process.utils.get_cross_correlation_fk(DP, probe_kernel_FT, corrPower)\n",
    "    cc = numpy.maximum(cc,0)\n",
    "    cc = scipy.ndimage.filters.gaussian_filter(cc, sigma)\n",
    "\n",
    "    # Get maxima\n",
    "    maxima_x,maxima_y,maxima_int = py4DSTEM.process.utils.get_maxima_2D(\n",
    "        cc, sigma=sigma, edgeBoundary=edgeBoundary,\n",
    "        minRelativeIntensity=minRelativeIntensity,\n",
    "        minSpacing=minPeakSpacing, maxNumPeaks=maxNumPeaks,\n",
    "        subpixel=subpixel)\n",
    "\n",
    "    # Make peaks PointList\n",
    "    if peaks is None:\n",
    "        coords = [('qx',float),('qy',float),('intensity',float)]\n",
    "        peaks = py4DSTEM.file.datastructure.PointList(coordinates=coords)\n",
    "    else:\n",
    "        assert(isinstance(peaks,py4DSTEM.file.datastructure.PointList))\n",
    "    peaks.add_tuple_of_nparrays((maxima_x,maxima_y,maxima_int))\n",
    "\n",
    "    if return_cc:\n",
    "        return peaks, cc\n",
    "    else:\n",
    "        return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_chunk(_f, start, end, path_to_static, coords, slices):\n",
    "    import pickle\n",
    "    #import numpy\n",
    "    \n",
    "    with open(path_to_static, 'rb') as infile:\n",
    "        inputs = pickle.load(infile)\n",
    "    \n",
    "    #dc = numpy.load(path_to_data, mmap_mode='r')\n",
    "\n",
    "    results = [(coords[n][0], coords[n][1], _f(slices[n], *inputs).data) for n in range(end - start)]\n",
    "    return results\n",
    "\n",
    "def _find_Bragg_disks(datacube, probe,\n",
    "                      corrPower = 1,\n",
    "                      sigma = 2,\n",
    "                      edgeBoundary = 20,\n",
    "                      minRelativeIntensity = 0.005,\n",
    "                      minPeakSpacing = 60,\n",
    "                      maxNumPeaks = 70,\n",
    "                      subpixel = True,\n",
    "                      verbose = False,\n",
    "                      cluster_id = None):\n",
    "    \"\"\"\n",
    "    Finds the Bragg disks in all diffraction patterns of datacube by cross, hybrid, or phase\n",
    "    correlation with probe.\n",
    "\n",
    "    Accepts:\n",
    "        DP                   (ndarray) a diffraction pattern\n",
    "        probe                (ndarray) the vacuum probe template, in real space.\n",
    "        corrPower            (float between 0 and 1, inclusive) the cross correlation power. A\n",
    "                             value of 1 corresponds to a cross correaltion, and 0 corresponds to a\n",
    "                             phase correlation, with intermediate values giving various hybrids.\n",
    "        sigma                (float) the standard deviation for the gaussian smoothing applied to\n",
    "                             the cross correlation\n",
    "        edgeBoundary         (int) minimum acceptable distance from the DP edge, in pixels\n",
    "        minRelativeIntensity (float) the minimum acceptable correlation peak intensity, relative to\n",
    "                             the intensity of the brightest peak\n",
    "        minPeakSpacing       (float) the minimum acceptable spacing between detected peaks\n",
    "        maxNumPeaks          (int) the maximum number of peaks to return\n",
    "        subpixel             (bool) if True, perform subpixel fitting to detected maxima\n",
    "        verbose              (bool) if True, prints completion updates\n",
    "\n",
    "    Returns:\n",
    "        peaks                (PointListArray) the Bragg peak positions and correlation intensities\n",
    "    \"\"\"\n",
    "    # Make the peaks PointListArray\n",
    "    coords = [('qx',float),('qy',float),('intensity',float)]\n",
    "    peaks = PointListArray(coordinates=coords, shape=(datacube.R_Nx, datacube.R_Ny))\n",
    "\n",
    "    # Get the probe kernel FT\n",
    "    probe_kernel_FT = np.conj(np.fft.fft2(probe))\n",
    "\n",
    "    if cluster_id is None:\n",
    "        # Loop over all diffraction patterns\n",
    "        t0 = time()\n",
    "        for Rx in range(datacube.R_Nx):\n",
    "            for Ry in range(datacube.R_Ny):\n",
    "                if verbose:\n",
    "                    print_progress_bar(Rx*datacube.R_Ny+Ry+1, datacube.R_Nx*datacube.R_Ny,\n",
    "                                       prefix='Analyzing:', suffix='Complete', length=50)\n",
    "                DP = datacube.data4D[Rx,Ry,:,:]\n",
    "                _find_Bragg_disks_single_DP_FK(DP, probe_kernel_FT,\n",
    "                                               corrPower = corrPower,\n",
    "                                               sigma = sigma,\n",
    "                                               edgeBoundary = edgeBoundary,\n",
    "                                               minRelativeIntensity = minRelativeIntensity,\n",
    "                                               minPeakSpacing = minPeakSpacing,\n",
    "                                               maxNumPeaks = maxNumPeaks,\n",
    "                                               subpixel = subpixel,\n",
    "                                               peaks = peaks.get_pointlist(Rx,Ry))\n",
    "        t = time()-t0\n",
    "        print(\"Analyzed {} diffraction patterns in {}h {}m {}s\".format(datacube.R_N, int(t/3600),\n",
    "                                                                       int(t/60), int(t%60)))\n",
    "    else:\n",
    "        t0 = time()\n",
    "        c = ipp.Client(cluster_id=cluster_id, timeout=30)\n",
    "        lastengine = len(c.ids) - 1\n",
    "\n",
    "        inputs_list = [\n",
    "            probe_kernel_FT,\n",
    "            corrPower,\n",
    "            sigma,\n",
    "            edgeBoundary,\n",
    "            minRelativeIntensity,\n",
    "            minPeakSpacing,\n",
    "            maxNumPeaks,\n",
    "            subpixel\n",
    "        ]\n",
    "        \n",
    "        t_00 = time()\n",
    "        # write out static inputs\n",
    "        path_to_inputs = os.path.join(os.path.expandvars(\"$SCRATCH\"), \"{}.inputs\".format(cluster_id))\n",
    "        with open(path_to_inputs, 'wb') as inputs_file:\n",
    "            pickle.dump(inputs_list, inputs_file)\n",
    "        t_inputs_save = time() - t_00\n",
    "        print(\"Serialize input values : {}\".format(t_inputs_save))\n",
    "        \n",
    "        #t_01 = time()\n",
    "        #path_to_data = os.path.join(os.path.expandvars(\"$SCRATCH\"), \"{}.datacube.npy\".format(cluster_id))\n",
    "        #with open(path_to_data, 'wb') as outfile:\n",
    "        #    np.save(outfile, datacube.data4D)\n",
    "        #t_datacube_save = time() - t_01\n",
    "        #print(\"Serialize datacube : {}\".format(t_datacube_save))\n",
    "        \n",
    "        #t_write_inputs = time() - t0        \n",
    "        #print(\"Serialize inputs phase : {}\".format(t_write_inputs))\n",
    "        \n",
    "        results = []        \n",
    "        t1 = time()\n",
    "        total = int(datacube.R_Nx * datacube.R_Ny)\n",
    "        chunkSize = int(total / len(c.ids))\n",
    "        \n",
    "        while chunkSize * len(c.ids) < total:\n",
    "            chunkSize += 1\n",
    "\n",
    "        indices = []\n",
    "        for Rx in range(datacube.R_Nx):\n",
    "            for Ry in range(datacube.R_Ny):\n",
    "                indices.append((Rx, Ry))\n",
    "        \n",
    "        start = 0\n",
    "        for engine in range(len(c.ids)):\n",
    "            if start + chunkSize < total - 1:\n",
    "                end = start + chunkSize\n",
    "            else:\n",
    "                end = total\n",
    "            \n",
    "            results.append(\n",
    "                c[engine].apply(\n",
    "                    _process_chunk,\n",
    "                    _find_Bragg_disks_single_DP_FK,\n",
    "                    start,\n",
    "                    end,\n",
    "                    path_to_inputs,\n",
    "                    indices[start:end],\n",
    "                    [datacube.data4D[x[0],x[1],:,:] for x in indices[start:end]]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            if end == total:\n",
    "                break\n",
    "            else:\n",
    "                start = end\n",
    "        t_submit = time() - t1\n",
    "        print(\"Submit phase : {}\".format(t_submit))\n",
    "        \n",
    "        t2 = time()\n",
    "        c.wait(jobs=results)\n",
    "        \n",
    "        for i in range(len(results)):\n",
    "            for Rx, Ry, data in results[i].get():\n",
    "                peaks.get_pointlist(Rx, Ry).add_dataarray(data.copy())\n",
    "        \n",
    "        t_copy = time() - t2\n",
    "        print(\"Gather phase : {}\".format(t_copy))\n",
    "        \n",
    "        t = time()-t0\n",
    "        print(\"Analyzed {} diffraction patterns in {}h {}m {}s\".format(datacube.R_N, int(t/3600),\n",
    "                                                                       int(t/60), int(t%60)))\n",
    "\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = open('slurm_jobid', 'r').read().strip()\n",
    "cluster_id = \"cori_{}\".format(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ipp.Client(cluster_id=cluster_id)\n",
    "print(c.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All DPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get peaks\n",
    "\n",
    "corrPower = 0.8\n",
    "sigma = 2\n",
    "edgeBoundary = 20\n",
    "maxNumPeaks = 70\n",
    "minPeakSpacing = 50\n",
    "minRelativeIntensity = 0.001\n",
    "verbose = False\n",
    "\n",
    "t0 = time()\n",
    "peaks = _find_Bragg_disks(dc, probe_kernel.data2D,\n",
    "                         corrPower=corrPower,\n",
    "                         sigma=sigma,\n",
    "                         edgeBoundary=edgeBoundary,\n",
    "                         minRelativeIntensity=minRelativeIntensity,\n",
    "                         minPeakSpacing=minPeakSpacing,\n",
    "                         maxNumPeaks=maxNumPeaks,\n",
    "                         verbose=verbose,\n",
    "                         cluster_id=cluster_id)\n",
    "overall_time = time() - t0\n",
    "print(overall_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "\n",
    "Rxs=(20,31,18)\n",
    "Rys=(25,31,10)\n",
    "power=0.3\n",
    "size_scale_factor = 500       # Set to zero to make all points the same size\n",
    "\n",
    "fig,((ax11,ax12),(ax21,ax22))=plt.subplots(2,2,figsize=(12,12))\n",
    "ax11.matshow(np.average(dc.data4D,axis=(2,3)))\n",
    "ax11.scatter(Rys,Rxs,color=('r','g','b'))\n",
    "ax12.matshow(dc.data4D[Rxs[0],Rys[0],:,:]**power)\n",
    "ax21.matshow(dc.data4D[Rxs[1],Rys[1],:,:]**power)\n",
    "ax22.matshow(dc.data4D[Rxs[2],Rys[2],:,:]**power)\n",
    "\n",
    "peaks0 = peaks.get_pointlist(Rxs[0],Rys[0])\n",
    "peaks1 = peaks.get_pointlist(Rxs[1],Rys[1])\n",
    "peaks2 = peaks.get_pointlist(Rxs[2],Rys[2])\n",
    "if size_scale_factor == 0:\n",
    "    ax12.scatter(peaks0.data['qy'],peaks0.data['qx'],color='r')\n",
    "    ax21.scatter(peaks1.data['qy'],peaks1.data['qx'],color='g')\n",
    "    ax22.scatter(peaks2.data['qy'],peaks2.data['qx'],color='b')\n",
    "else:\n",
    "    ax12.scatter(peaks0.data['qy'],peaks0.data['qx'],color='r',s=size_scale_factor*peaks0.data['intensity']/np.max(peaks0.data['intensity']))\n",
    "    ax21.scatter(peaks1.data['qy'],peaks1.data['qx'],color='g',s=size_scale_factor*peaks1.data['intensity']/np.max(peaks1.data['intensity']))\n",
    "    ax22.scatter(peaks2.data['qy'],peaks2.data['qx'],color='b',s=size_scale_factor*peaks2.data['intensity']/np.max(peaks2.data['intensity']))\n",
    "\n",
    "ax11.axis('off')\n",
    "ax12.axis('off')\n",
    "ax21.axis('off')\n",
    "ax22.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply post-detection thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove points based on new peak spacing or minimum relative intensity thresholds\n",
    "\n",
    "maxNumPeaks = 20\n",
    "minPeakSpacing = 50\n",
    "minRelativeIntensity = 0.01\n",
    "\n",
    "peaks_thresh = peaks.copy(name='Braggpeaks')  # Create a copy of the PointListArray to further threshold\n",
    "peaks_thresh = threshold_Braggpeaks(peaks_thresh,\n",
    "                                    minRelativeIntensity=minRelativeIntensity,\n",
    "                                    minPeakSpacing=minPeakSpacing,\n",
    "                                    maxNumPeaks=maxNumPeaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "\n",
    "Rxs=(20,31,18)\n",
    "Rys=(25,31,10)\n",
    "power=0.3\n",
    "size_scale_factor = 500       # Set to zero to make all points the same size\n",
    "\n",
    "fig,((ax11,ax12),(ax21,ax22))=plt.subplots(2,2,figsize=(12,12))\n",
    "ax11.matshow(np.average(dc.data4D,axis=(2,3)))\n",
    "ax11.scatter(Rys,Rxs,color=('r','g','b'))\n",
    "ax12.matshow(dc.data4D[Rxs[0],Rys[0],:,:]**power)\n",
    "ax21.matshow(dc.data4D[Rxs[1],Rys[1],:,:]**power)\n",
    "ax22.matshow(dc.data4D[Rxs[2],Rys[2],:,:]**power)\n",
    "\n",
    "peaks0 = peaks_thresh.get_pointlist(Rxs[0],Rys[0])\n",
    "peaks1 = peaks_thresh.get_pointlist(Rxs[1],Rys[1])\n",
    "peaks2 = peaks_thresh.get_pointlist(Rxs[2],Rys[2])\n",
    "if size_scale_factor == 0:\n",
    "    ax12.scatter(peaks0.data['qy'],peaks0.data['qx'],color='r')\n",
    "    ax21.scatter(peaks1.data['qy'],peaks1.data['qx'],color='g')\n",
    "    ax22.scatter(peaks2.data['qy'],peaks2.data['qx'],color='b')\n",
    "else:\n",
    "    ax12.scatter(peaks0.data['qy'],peaks0.data['qx'],color='r',s=size_scale_factor*peaks0.data['intensity']/np.max(peaks0.data['intensity']))\n",
    "    ax21.scatter(peaks1.data['qy'],peaks1.data['qx'],color='g',s=size_scale_factor*peaks1.data['intensity']/np.max(peaks1.data['intensity']))\n",
    "    ax22.scatter(peaks2.data['qy'],peaks2.data['qx'],color='b',s=size_scale_factor*peaks2.data['intensity']/np.max(peaks2.data['intensity']))\n",
    "\n",
    "ax11.axis('off')\n",
    "ax12.axis('off')\n",
    "ax21.axis('off')\n",
    "ax22.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py4dstem",
   "language": "python",
   "name": "py4dstem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
