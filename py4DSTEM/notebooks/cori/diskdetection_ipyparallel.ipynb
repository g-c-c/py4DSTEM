{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module: braggdiskdetection\n",
    "\n",
    "This module contains functions finding the positions of the Bragg disks in a 4DSTEM scan.  Generally this will involve two steps: getting a vacuum probe, then finding the Bragg disks using the vacuum probe as a template. \n",
    "\n",
    "## Submodule: diskdetection\n",
    "\n",
    "The notebook demos functions related to finding the Bragg disks.  Using a vacuum probe as a template - i.e. a convolution kernel - a cross correlation (or phase or hybrid correlation) is taken between each DP and the template, and the positions and intensities of all local corraltion maxima are used to identify the Bragg disks.  Erroneous peaks are filtered out with several types of threshold.  Detected Bragg disks are generally stored in PointLists (when run on only selected DPs) or PointListArrays (when run on a full DataCube).\n",
    "\n",
    "This notebook demos:\n",
    "* Disk detection on single or selected diffraction patterns\n",
    "* Disk detection on all diffraction patterns\n",
    "* Additional filtering of detected Bragg disks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages, load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py4DSTEM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from py4DSTEM.process.braggdiskdetection import find_Bragg_disks_single_DP\n",
    "from py4DSTEM.process.braggdiskdetection import find_Bragg_disks_selected\n",
    "from py4DSTEM.process.braggdiskdetection import find_Bragg_disks\n",
    "from py4DSTEM.process.braggdiskdetection import threshold_Braggpeaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py4DSTEM.disk_detection_parallel\n",
    "\n",
    "from py4DSTEM.disk_detection_parallel import find_Bragg_disks_ipp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "#fp = \"/home/ben/Data/20180905_FePO4_unlithiated/raw/Stack1_57x47+30nmss_spot 8_0p05s_CL=600_alpha=0p48_300kV_bin4.dm4\"\n",
    "#fp = \"/Users/Ben/Work/NCEM/Projects/py4DSTEM/sample_data/20180905_FePO4_unlithiated/Stack2_60x60+30nmss_spot 8_0p05s_CL=600_alpha=0p48_300kV_bin4.dm3\"\n",
    "\n",
    "#fp = \"/global/project/projectdirs/ncemhub/py4DSTEM-test-data/Cuscan7.dm4\"\n",
    "fp = \"/global/u2/m/mhenders/ncem/Stack2_60x60+30nmss_spot 8_0p05s_CL=600_alpha=0p48_300kV_bin4.h5\"\n",
    "dc = py4DSTEM.file.io.read(fp)\n",
    "dc.set_scan_shape(47,57)\n",
    "#dc.data4D = np.roll(dc.data4D,-2,1) # Correct for acquisition wrap-around error\n",
    "\n",
    "# Load the template\n",
    "#fp_probetemplate = \"/home/ben/Data/20180905_FePO4_unlithiated/processing/vacuum_probe_kernel.h5\"\n",
    "#fp = \"/Users/Ben/Work/NCEM/Projects/py4DSTEM/sample_data/20180905_FePO4_unlithiated/processing/vacuum_probe_kernel.h5\"\n",
    "fp_probetemplate = \"/global/u2/m/mhenders/ncem/vacuum_probe_kernel.h5\"\n",
    "browser = py4DSTEM.file.io.FileBrowser(fp_probetemplate, rawdatacube=dc)\n",
    "browser.show_dataobjects()\n",
    "probe_kernel = browser.get_dataobject(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a DP\n",
    "\n",
    "Rx=20\n",
    "Ry=25\n",
    "power=0.3\n",
    "\n",
    "DP = dc.data4D[Rx,Ry,:,:]\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,12))\n",
    "ax1.matshow(np.average(dc.data4D,axis=(2,3)))\n",
    "ax1.scatter(Ry,Rx,color='r')\n",
    "ax2.matshow(DP**power)\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get peaks\n",
    "\n",
    "corrPower = 0.9\n",
    "sigma = 2\n",
    "edgeBoundary = 20\n",
    "maxNumPeaks = 70\n",
    "minPeakSpacing = 30\n",
    "minRelativeIntensity = 0.005\n",
    "\n",
    "peaks = find_Bragg_disks_single_DP(DP, probe_kernel.data2D,\n",
    "                                   corrPower=corrPower,\n",
    "                                   sigma=sigma,\n",
    "                                   edgeBoundary=edgeBoundary,\n",
    "                                   minRelativeIntensity=minRelativeIntensity,\n",
    "                                   minPeakSpacing=minPeakSpacing,\n",
    "                                   maxNumPeaks=maxNumPeaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "\n",
    "power=0.3\n",
    "size_scale_factor = 500       # Set to zero to make all points the same size\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,12))\n",
    "ax1.matshow(np.average(dc.data4D,axis=(2,3)))\n",
    "ax1.scatter(Ry,Rx,color='r')\n",
    "ax2.matshow(DP**power)\n",
    "ax2.scatter(peaks.data['qy'],peaks.data['qx'],color='r',s=size_scale_factor*peaks.data['intensity']/np.max(peaks.data['intensity']))\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Several DPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few DPs\n",
    "\n",
    "Rxs=(20,31,18)\n",
    "Rys=(25,31,10)\n",
    "power=0.3\n",
    "\n",
    "fig,((ax11,ax12),(ax21,ax22))=plt.subplots(2,2,figsize=(12,12))\n",
    "ax11.matshow(np.average(dc.data4D,axis=(2,3)))\n",
    "ax11.scatter(Rys,Rxs,color=('r','yellow','deepskyblue'))\n",
    "ax12.matshow(dc.data4D[Rxs[0],Rys[0],:,:]**power)\n",
    "ax21.matshow(dc.data4D[Rxs[1],Rys[1],:,:]**power)\n",
    "ax22.matshow(dc.data4D[Rxs[2],Rys[2],:,:]**power)\n",
    "\n",
    "ax11.axis('off')\n",
    "ax12.axis('off')\n",
    "ax21.axis('off')\n",
    "ax22.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get peaks\n",
    "\n",
    "corrPower = 0.8\n",
    "sigma = 2\n",
    "edgeBoundary = 20\n",
    "maxNumPeaks = 70\n",
    "minPeakSpacing = 50\n",
    "minRelativeIntensity = 0.001\n",
    "\n",
    "peaks = find_Bragg_disks_selected(dc, probe_kernel.data2D, Rxs, Rys,\n",
    "                                  corrPower=corrPower,\n",
    "                                  sigma=sigma,\n",
    "                                  edgeBoundary=edgeBoundary,\n",
    "                                  minRelativeIntensity=minRelativeIntensity,\n",
    "                                  minPeakSpacing=minPeakSpacing,\n",
    "                                  maxNumPeaks=maxNumPeaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "\n",
    "power=0.3\n",
    "size_scale_factor = 500       # Set to zero to make all points the same size\n",
    "\n",
    "fig,((ax11,ax12),(ax21,ax22))=plt.subplots(2,2,figsize=(12,12))\n",
    "ax11.matshow(np.average(dc.data4D,axis=(2,3)))\n",
    "ax11.scatter(Rys,Rxs,color=('r','g','b'))\n",
    "ax12.matshow(dc.data4D[Rxs[0],Rys[0],:,:]**power)\n",
    "ax21.matshow(dc.data4D[Rxs[1],Rys[1],:,:]**power)\n",
    "ax22.matshow(dc.data4D[Rxs[2],Rys[2],:,:]**power)\n",
    "\n",
    "if size_scale_factor == 0:\n",
    "    ax12.scatter(peaks[0].data['qy'],peaks[0].data['qx'],color='r')\n",
    "    ax21.scatter(peaks[1].data['qy'],peaks[1].data['qx'],color='g')\n",
    "    ax22.scatter(peaks[2].data['qy'],peaks[2].data['qx'],color='b')\n",
    "else:\n",
    "    ax12.scatter(peaks[0].data['qy'],peaks[0].data['qx'],color='r',s=size_scale_factor*peaks[0].data['intensity']/np.max(peaks[0].data['intensity']))\n",
    "    ax21.scatter(peaks[1].data['qy'],peaks[1].data['qx'],color='g',s=size_scale_factor*peaks[1].data['intensity']/np.max(peaks[1].data['intensity']))\n",
    "    ax22.scatter(peaks[2].data['qy'],peaks[2].data['qx'],color='b',s=size_scale_factor*peaks[2].data['intensity']/np.max(peaks[2].data['intensity']))\n",
    "\n",
    "\n",
    "ax11.axis('off')\n",
    "ax12.axis('off')\n",
    "ax21.axis('off')\n",
    "ax22.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = open('slurm_jobid', 'r').read().strip()\n",
    "cluster_id = \"cori_{}\".format(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "c = ipp.Client(cluster_id=cluster_id)\n",
    "print(c.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All DPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get peaks\n",
    "from time import time\n",
    "\n",
    "corrPower = 0.8\n",
    "sigma = 2\n",
    "edgeBoundary = 20\n",
    "maxNumPeaks = 70\n",
    "minPeakSpacing = 50\n",
    "minRelativeIntensity = 0.001\n",
    "verbose = False\n",
    "subpixel = 'poly'\n",
    "#subpixel = 'multicorr'\n",
    "#upsample_factor=16\n",
    "\n",
    "t0 = time()\n",
    "peaks = find_Bragg_disks_ipp(\n",
    "    dc, probe_kernel.data2D,\n",
    "    corrPower=corrPower,\n",
    "    sigma=sigma,\n",
    "    edgeBoundary=edgeBoundary,\n",
    "    minRelativeIntensity=minRelativeIntensity,\n",
    "    minPeakSpacing=minPeakSpacing,\n",
    "    maxNumPeaks=maxNumPeaks,\n",
    "    verbose=verbose,\n",
    "    subpixel=subpixel,\n",
    "    #upsample_factor=upsample_factor,\n",
    "    cluster_id=cluster_id)\n",
    "overall_time = time() - t0\n",
    "print(overall_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "\n",
    "Rxs=(20,31,18)\n",
    "Rys=(25,31,10)\n",
    "power=0.3\n",
    "size_scale_factor = 500       # Set to zero to make all points the same size\n",
    "\n",
    "fig,((ax11,ax12),(ax21,ax22))=plt.subplots(2,2,figsize=(12,12))\n",
    "ax11.matshow(np.average(dc.data4D,axis=(2,3)))\n",
    "ax11.scatter(Rys,Rxs,color=('r','g','b'))\n",
    "ax12.matshow(dc.data4D[Rxs[0],Rys[0],:,:]**power)\n",
    "ax21.matshow(dc.data4D[Rxs[1],Rys[1],:,:]**power)\n",
    "ax22.matshow(dc.data4D[Rxs[2],Rys[2],:,:]**power)\n",
    "\n",
    "peaks0 = peaks.get_pointlist(Rxs[0],Rys[0])\n",
    "peaks1 = peaks.get_pointlist(Rxs[1],Rys[1])\n",
    "peaks2 = peaks.get_pointlist(Rxs[2],Rys[2])\n",
    "if size_scale_factor == 0:\n",
    "    ax12.scatter(peaks0.data['qy'],peaks0.data['qx'],color='r')\n",
    "    ax21.scatter(peaks1.data['qy'],peaks1.data['qx'],color='g')\n",
    "    ax22.scatter(peaks2.data['qy'],peaks2.data['qx'],color='b')\n",
    "else:\n",
    "    ax12.scatter(peaks0.data['qy'],peaks0.data['qx'],color='r',s=size_scale_factor*peaks0.data['intensity']/np.max(peaks0.data['intensity']))\n",
    "    ax21.scatter(peaks1.data['qy'],peaks1.data['qx'],color='g',s=size_scale_factor*peaks1.data['intensity']/np.max(peaks1.data['intensity']))\n",
    "    ax22.scatter(peaks2.data['qy'],peaks2.data['qx'],color='b',s=size_scale_factor*peaks2.data['intensity']/np.max(peaks2.data['intensity']))\n",
    "\n",
    "ax11.axis('off')\n",
    "ax12.axis('off')\n",
    "ax21.axis('off')\n",
    "ax22.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply post-detection thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove points based on new peak spacing or minimum relative intensity thresholds\n",
    "\n",
    "maxNumPeaks = 20\n",
    "minPeakSpacing = 50\n",
    "minRelativeIntensity = 0.01\n",
    "\n",
    "peaks_thresh = peaks.copy(name='Braggpeaks')  # Create a copy of the PointListArray to further threshold\n",
    "peaks_thresh = threshold_Braggpeaks(peaks_thresh,\n",
    "                                    minRelativeIntensity=minRelativeIntensity,\n",
    "                                    minPeakSpacing=minPeakSpacing,\n",
    "                                    maxNumPeaks=maxNumPeaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "\n",
    "Rxs=(20,31,18)\n",
    "Rys=(25,31,10)\n",
    "power=0.3\n",
    "size_scale_factor = 500       # Set to zero to make all points the same size\n",
    "\n",
    "fig,((ax11,ax12),(ax21,ax22))=plt.subplots(2,2,figsize=(12,12))\n",
    "ax11.matshow(np.average(dc.data4D,axis=(2,3)))\n",
    "ax11.scatter(Rys,Rxs,color=('r','g','b'))\n",
    "ax12.matshow(dc.data4D[Rxs[0],Rys[0],:,:]**power)\n",
    "ax21.matshow(dc.data4D[Rxs[1],Rys[1],:,:]**power)\n",
    "ax22.matshow(dc.data4D[Rxs[2],Rys[2],:,:]**power)\n",
    "\n",
    "peaks0 = peaks_thresh.get_pointlist(Rxs[0],Rys[0])\n",
    "peaks1 = peaks_thresh.get_pointlist(Rxs[1],Rys[1])\n",
    "peaks2 = peaks_thresh.get_pointlist(Rxs[2],Rys[2])\n",
    "if size_scale_factor == 0:\n",
    "    ax12.scatter(peaks0.data['qy'],peaks0.data['qx'],color='r')\n",
    "    ax21.scatter(peaks1.data['qy'],peaks1.data['qx'],color='g')\n",
    "    ax22.scatter(peaks2.data['qy'],peaks2.data['qx'],color='b')\n",
    "else:\n",
    "    ax12.scatter(peaks0.data['qy'],peaks0.data['qx'],color='r',s=size_scale_factor*peaks0.data['intensity']/np.max(peaks0.data['intensity']))\n",
    "    ax21.scatter(peaks1.data['qy'],peaks1.data['qx'],color='g',s=size_scale_factor*peaks1.data['intensity']/np.max(peaks1.data['intensity']))\n",
    "    ax22.scatter(peaks2.data['qy'],peaks2.data['qx'],color='b',s=size_scale_factor*peaks2.data['intensity']/np.max(peaks2.data['intensity']))\n",
    "\n",
    "ax11.axis('off')\n",
    "ax12.axis('off')\n",
    "ax21.axis('off')\n",
    "ax22.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py4dstem",
   "language": "python",
   "name": "py4dstem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
